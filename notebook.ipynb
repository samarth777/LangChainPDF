{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = \"llama2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure! Here's one:\\n\\nWhy don't scientists trust atoms?\\nBecause they make up everything!\\n\\nI hope you found that amusing! Do you want to hear another one?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings()\n",
    "model.invoke(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='www.pes.edu \\n \\n \\n \\n \\n \\n \\n \\nDivide and Conquer Approach \\nBinary Search \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nDr. Shylaja S S Department of Computer Science and Engineering \\nPES UNIVERSITY \\nUE19CS251 : Design and Analysis of Algorithms (4-0-0-4- 4)', metadata={'source': 'DAA.pdf', 'page': 0}),\n",
       " Document(page_content=\"www.pes.edu \\nDivide-and-C onquer Approach \\nDivide-and-Conquer is probably the best-known general algorithm design  \\ntechnique. Though its fame may have something to do with its catchy name, it \\nis well deserved: quite a few very efficient algorithms are specific \\nimplementations of this general strategy.   Divide-and-conquer algorithms work \\naccording to the following general plan:  \\n1.  A  problem's  instance  is  divided  into  several  smaller  instanc es  of the  \\nsame problem, ideally of about the same size.  \\n2.  The smaller instances are solved (typically recursively, though sometimes a \\ndifferent algorithm is employed when instances become small enough).  \\n3.  If necessary, the solutions obtained for the smaller instances are combined  \\nto get a solution to the original instance.  \\n \\nThe divide-and-conquer technique is diagrammed in Fig. 1, which depicts the \\ncase of dividing a problem into two smaller sub problems, by far the most \\nwidely occurring case. \\n \\n \\n \\nFig. 1: Divide-and-conquer technique (typical case)\", metadata={'source': 'DAA.pdf', 'page': 1}),\n",
       " Document(page_content='www.pes.edu \\nGeneral Divide and Conquer Recurrence \\nIn the most typical cases of Divide and Conquer, a problem’s instance of size n \\ncan be divided into b instances of size n/b, with a of them needing to be \\nsolved . Here a and b are constants; a >= 1 and b >= 1 . Assuming that size n is a \\npower of b, we get the following recurrence for the running time: \\nT(n) = a * T(n/b) + f(n) \\nf(n) is a function that accounts for the time spent on dividing the problem and \\ncombining the solutions . The efficiency analysis of many divide-and-conquer \\nalgorithms is greatly simplified by the following theorem. \\n \\nMaster Theorem \\nFor the recurrence:  \\n   T(n) = a * T(n/b) + f(n) \\nIf f(n) ∈ Θ(nd), where d >= 0 in the recurrence relation, then: \\nIf a < bd,    T(n) ∈ Θ (nd)  \\nIf a = bd,    T(n)  ∈ Θ (nd log n)  \\nIf a > bd,    T(n) ∈ Θ (nlog b a\\n)  \\n \\n Analogous results hold for O and Ω as well!  \\n \\nBinary Search \\nBinary Search is a remarkably efficient algorithm for searching in a sorted \\narray . It works by comparing the search key K with the array’s middle element \\nA[m] . If they match, the algorithm stops . Otherwise, the same operation is \\nrepeated recursively for the first half of the array if K < A[m] and for the second \\nhalf if K > A[m]. \\n \\nAs an example, let us apply binary search to searching for K = 70 in the array. \\nThe iterations of the algorithm are given in the following table.', metadata={'source': 'DAA.pdf', 'page': 2}),\n",
       " Document(page_content=\"www.pes.edu \\nThough  binary search is  clearly  based  on a  recursive  idea,  it  can  be  easily \\nimplemented  as  a  non recursive  algorithm,  too.  Here is a pseudo code for \\nthis non recursive version. \\nALGORITHM  BinarySearch(A[0 .. n  -1], K)  \\n// Implements non recursive binary search  \\n// Input: An array A [0 ... n - 1] sorted in   ascending order and a search ke y K  \\n// Output: An index of the array's element that is equal to K or -1 if there is no \\n//such element  \\nl \\uf0df 0; r \\uf0df n-1  \\nwhile l ≤ r do  \\n 𝑚 ← ⌊(𝑙 + 𝑟)/2 ⌋ \\n if K = A[m] return m \\n else if K < A[m]  r \\uf0df m-1  \\n else l \\uf0dfm+1  \\nreturn -1 \\n \\nBinary Search Analysis \\nWorst Case: The basic operation is the comparison of the search key with an \\nelement of the array . The number of comparisons made is given by the \\nfollowing recurrence: \\n𝑪𝒘𝒐𝒓𝒔𝒕 (𝐧)= 𝑪 𝒘𝒐𝒓𝒔𝒕 (⌊𝐧𝟐⁄⌋)+  𝟏  𝐟𝐨𝐫 𝐧 > 1, 𝐂 𝐰𝐨𝐫𝐬𝐭 (𝟏)= 𝟏 \\nFor the initial condition C worst(1) = 1, we obtain: \\n𝑪𝒘𝒐𝒓𝒔𝒕 (𝟐𝒌)= 𝒌 + 𝟏 = 𝐥𝐨𝐠 𝟐𝒏 + 𝟏  \\nFor any arbitrary positive integer, n: \\n𝑪𝒘𝒐𝒓𝒔𝒕 (𝒏)=  ⌊𝐥𝐨𝐠 𝟐𝒏⌋+  𝟏  \\n \\nAverage Case:  \\n𝑪𝒂𝒗𝒈 ≈  𝐥𝐨𝐠 𝟐𝒏\", metadata={'source': 'DAA.pdf', 'page': 3}),\n",
       " Document(page_content='www.pes.edu \\n \\n \\n \\n \\n \\n \\n \\nMerge Sort \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nDr. Shylaja S S Department of Computer Science and Engineering \\nPES UNIVERSITY \\nUE19CS251 : Design and Analysis of Algorithms (4-0-0-4- 4)', metadata={'source': 'DAA.pdf', 'page': 4}),\n",
       " Document(page_content='www.pes.edu \\nMerge Sort \\nMergesort  is  a  perfect  example  of  a  successful  application  of  the  d ivide-\\nand-conquer technique. It sorts a given array A[0 .. n  - 1] by dividing it into tw o \\nhalves A[0 ..  ⌊𝑛2⁄⌋ -1] and A[ ⌊𝑛2⁄⌋ … n-1], sorting  each  of them \\nrecursively,  and  then merging the two smaller sorted arrays into a single \\nsorted one. \\nALGORITHM  Mergesort(A[0 .. n - 1])  \\n//Sorts array A[0 .. n  - 1] by recursive mergesort  \\n//Input:  An array A[0 .. n  - 1]  of orderable elements  \\n//Output: Array A[0 .. n  - 1] sorted in nondecreasing order  \\nif n >  1  \\ncopy A[0 .. ⌊𝑛2⁄⌋ -1] to B[0 .. ⌊𝑛2⁄⌋ -1]  \\ncopy A[ ⌊𝑛2⁄⌋.. n -1] to C[0 .. ⌈𝑛2⁄⌉ -1] \\nMergesort(B[0 .. ⌊𝑛2⁄⌋ - 1])  \\nMergesort(C[0 .. ⌈𝑛2⁄⌉ -1])  \\nMerge(B,  C,  A) \\n \\nThe merging of two sorted arrays can be done as follows. Two pointers (array \\nindices) are initialized to point to the first elements of the arrays being merged.  \\nThe elements pointed to are compared, and the smaller of them is added to a  \\nnew array being constructed; after that, the index of the smaller element is \\nincremented to point to its immediate successor in the array it was copied \\nfrom. This operation is repeated until one of the two given arrays is exhausted, \\nand then the remaining elements of the other array are copied to the end of \\nthe new array. \\nALGORITHM  Merge(B[0 .. p- 1],  C[0 .. q  -1], A[0 .. p  + q  -1])  \\n//Merges two sorted arrays into one sorted array  \\n//Input: Arrays B[0 .. p -1] and C[0 .. q -1] both sorted  \\n//Output: Sorted array A[0 .. p  + q  -1] of the elements of B and C  \\ni\\uf0df0;  j\\uf0df0;  k\\uf0df0  \\nwhile i < p and j < q do  \\n if B[i] ≤ C[j]  \\n  A[k] \\uf0df B[i];  i \\uf0df i  + 1  \\n else  A[k] \\uf0dfC[j];  j\\uf0df j  + 1  \\n k\\uf0dfk+1', metadata={'source': 'DAA.pdf', 'page': 5}),\n",
       " Document(page_content='www.pes.edu \\nif i =  p  \\n copy C[j .. q-1] to A[k .. p + q - 1]  \\nelse \\n copy B[i .. p -1] to A[k .. p + q - 1] \\n \\nThe operation of the algorithm on the list  8,  3,  2,  9,  7,  1,  5,  4 is illustrated \\nin Fig. 1. \\n \\nFig. 1: Example of mergesort operation \\n \\nAssuming for simplicity that n is a power of 2, the recurrence relation for the \\nnumber of key comparisons C(n) is: \\nC(n) = 2C(n/2) + C merge(n) [for n > 1], C(1) = 0 \\n \\nThe number of key comparisons performed during the merging stage in the \\nworst case is:  \\nCmerge(n) = n – 1 \\nUsing the above equation: \\nCworst(n) = 2C worst(n/2) + n – 1 [for n > 1], C worst(1) = 0 \\n \\nApplying Master Theorem to the above equation: \\nCworst(n) ∈ Θ (n log n)', metadata={'source': 'DAA.pdf', 'page': 6}),\n",
       " Document(page_content='www.pes.edu \\n \\n \\n \\n \\n \\n \\n \\nQuick Sort \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nDr. Shylaja S S Department of Computer Science and Engineering \\nPES UNIVERSITY \\nUE19CS251 : Design and Analysis of Algorithms (4-0-0-4- 4)', metadata={'source': 'DAA.pdf', 'page': 7}),\n",
       " Document(page_content=\"www.pes.edu \\nQuick Sort \\nQuicksort is another important sorting algorithm that is based on the divide-\\nand-conquer approach. Unlike mergesort, which divides its input's element s \\naccording to  their  position  in  the  array,  quicksort  divides  them accord ing  \\nto  their  value. Specifically,  it  rearranges  elements  of  a  given  array           \\nA[0 .. n - 1]  to  achieve  its partition, a situation  where all the elements before \\nsome position s  are  smaller than or equal to A[s] and all the elements after \\npositions are greater than or equal to A[s]: \\n \\n \\nObviously, after a partition has been achieved, A[s] will be in its final posi tion \\nin the sorted array,  and we   can continue sorting the two sub arrays of the \\nelements preceding and following A[s] independently (e.g., by the same \\nmethod). \\n \\nALGORITHM  Quicksort(A[l .. r])  \\n// Sorts a subarray by quicksort  \\n// Input: A subarray A[l … r] of A[0 .. n  -1], defined by its left and right indices l  \\n//and r  \\n// Output: Subarray A[l .. r]  sorted in non decreasing order \\nif l  <  r  \\n s \\uf0dfPartition(A[l .. r])  //s is a split position  \\n Quicksort(A[l  .. s - 1])  \\n Quicksort(A[s + 1 .. r]) \\n \\nA partition of A [0 ... n- 1] and, more generally, of its subarray A[l .. r]                \\n(0 ≤  l  < r ≤ n - 1) can be achieved by the following algorithm. First, we select \\nan element with respect to whose value we are going to divide the subarray.  \\nBecause of its guiding role, we call this element the pivot. There are several \\ndifferent strategies for selecting a pivot. For now, we use the simplest strategy \\nof selecting the subarray's first element: p  =  A[l].  \\n \\nThere are also several alternative procedures for rearranging elements to \\nachieve a partition.  Here we use an efficient method based on two scans of\", metadata={'source': 'DAA.pdf', 'page': 8}),\n",
       " Document(page_content=\"www.pes.edu \\nthe subarray: one is le ft-to-right and the other right- to-left, each comparing \\nthe sub-array's elements with the pivot. The left- to-right scan, denoted below \\nby index i, starts with the second element. Since we want elements smaller \\nthan the pivot to be in the first part of the subarray, this scan skips over \\nelements that are smaller than the pivot and stops on encountering the first \\nelement greater than or equal to the pivot. The right- to-left scan, denoted \\nbelow by index j, starts with the last element of the subarray.  Since w e want \\nelements larger than the pivot to be in the second part of the sub array, this \\nscan skips over elements that are larger than the pivot and stops on \\nencountering the first element smaller than or equal to the pivot.  \\nAfter both scans stop, three situations may arise, depending on whether or not \\nthe scanning indices have crossed. If scanning indices i and j have not cross ed, \\ni.e., i < j, we simply exchange A[i] and A[j] and resume the scans by \\nincrementing i and decrementing j, respectively: \\n \\nIf the scanning indices have crossed over, i.e., i >  j, we will have partitioned \\nthe array after exchanging the pivot with A[j ]: \\n \\n \\nFinally, if the scanning indices stop while pointing to the same element, i. e.,      \\ni = j, the value they are pointing to must be equal top (why?). Thus, we have \\nthe array partitioned, with the split positions s = i  = j: \\n \\nWe can combine the last case with the case of crossed-over indices (i  >  j) by \\nexchanging the pivot with A[j] whenever i ≥j.  \\n \\nHere is a pseudocode implementing this partitioning procedure.\", metadata={'source': 'DAA.pdf', 'page': 9}),\n",
       " Document(page_content='www.pes.edu \\nALGORITHM  Partition(A[l .. r])  \\n// Partitions a subarray by using its first element as a pivot  \\n// Input: A  subarray A[l..r] of A[0 .. n  - 1],  defined by its left and right  in dices l \\n// and r  (l  <  r)  \\n// Output: A partition of A[l .. r ], with the split position returned as thi s  \\n//function\\'s value  \\np \\uf0dfA[l]  \\ni  \\uf0df l;  j \\uf0df r  + 1  \\nrepeat  \\n repeat i \\uf0df i + 1 until A[i] ≥ p  \\n repeat j \\uf0df j  - 1 until A[j] ≤ p  \\n swap(A[i],  A[j])  \\nuntil i ≥ j  \\nswap(A[i],  A[j])   //undo last swap when i ≥ j   \\nswap(A[i],  A[j])  \\nreturn  j \\n \\nNote that index i can go out of the subarray bounds in this pseudocode. Rather \\nthan checking for this possibility every time index i is incremented, we can \\nappend to array A[0 .. n- 1] a \"sentinel\" that would prevent index i from \\nadvancing beyond position n. The more sophisticated method of pivot \\nselection mentioned at the end of the section makes such a sentinel \\nunnecessary.', metadata={'source': 'DAA.pdf', 'page': 10}),\n",
       " Document(page_content=\"www.pes.edu \\nAn example of sorting an array by quicksort is given in Fig. 1. \\n \\nFig. 1: Example of Quicksort operation.  (a)  The array's transformations with \\npivots shown in bold.  (b) The tree of recursive calls to Quicksort with input \\nvalues l and r of subarray bounds and split positions of a partition obtained. \\nQuick sort Analysis: \\nBest Case: The number of comparisons in the best case satisfies the \\nrecurrence: \\nCbest (n) = 2C best (n/2) + n  for n > 1,  C best(1) = 0 \\n \\nAccording to Master Theorem \\n𝐶𝑏𝑒𝑠𝑡(𝑛)∈  Θ(𝑛 log 2𝑛) \\nWorst Case: The number of comparisons in the worst case satisfies the \\nrecurrence: \\nCworst (n)=(n + 1 )+  n + ⋯ + 3 =(n + 1 )(n + 2)\\n2− 3 ∈ θ(n2) \\nAverage Case: Let C avg(n) be the number of key comparisons made by Quick \\nSort on a randomly ordered array of size n. \\n𝐶𝑎𝑣𝑔(𝑛)=  1\\n𝑛 ∑[(𝑛 + 1 )+ 𝐶 𝑎𝑣𝑔𝑛−1\\n𝑠=0(𝑠)+ 𝐶 𝑎𝑣𝑔(𝑛 − 1 − 𝑠)] 𝑓𝑜𝑟 𝑛 > 1  \\nThe solution for the above recurrence is: \\n𝐶𝑎𝑣𝑔(𝑛)≈ 2𝑛 ln𝑛 ≈ 1. 38𝑛 log 2𝑛\", metadata={'source': 'DAA.pdf', 'page': 11}),\n",
       " Document(page_content='www.pes.edu \\n \\n \\n \\n \\n \\n \\n \\nBinary Tree \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nDr. Shylaja S S Department of Computer Science and Engineering \\nPES UNIVERSITY \\nUE19CS251 : Design and Analysis of Algorithms (4-0-0-4- 4)', metadata={'source': 'DAA.pdf', 'page': 12}),\n",
       " Document(page_content=\"www.pes.edu \\nBinary Tree \\nA binary tree T is defined as a finite set of nodes that is either empty or \\nconsists of a root and two disjoint binary trees T L and T R called as the left and \\nright subtree of the root . The definition itself divides the Binary Tree into two \\nsmaller structures and hence many problems concerning the binary trees can \\nbe solved using the Divide – And – Conquer technique . The binary tree is a \\nDivide – And – Conquer ready structure. \\n \\n \\nFig. 1: Standard representation of a Binary Tree \\n \\nHeight of a Binary Tree: Length of the longest path from root to leaf \\nALGORITHM  Height(T)  \\n//Computes recursively the height of a binary tree  \\n//Input: A binary tree T  \\n//Output: The height of T \\nif T =  Ø  return - 1  \\nelse return max(Height(T L), Height(T R))+ 1 \\n \\nHeight of a Binary Tree Analysis: \\nWe  measure  the  problem's  instance  size  by the  number of nodes n(T)  in  a \\ngiven binary  tree  T.  Obviously, the number of comparisons made to compute \\nthe maximum of two numbers and the number of additions A(n(T)) made by \\nthe algorithm are the same. \\nWe have the following recurrence relation for A(n(T)):  \\nA(n(T)) =  A(n(T L)) + A(n(T R)) + 1  for n(T) >  0,  \\nA(0) =  0. \\n  \\nBefore we solve this recurrence (can you tell what its solution is?), let us note  \\nthat addition is not the most frequently executed operation of this algorithm. \\nWhat is?  Checking-and this is very typical for binary tree algorithms-that t he\", metadata={'source': 'DAA.pdf', 'page': 13}),\n",
       " Document(page_content=\"www.pes.edu \\ntree is not empty.  For example,  for  the  empty  tree,  the  comparison  T  =  Ø  \\nis  executed once but there are no additions,  and for  a single-node tree,  the  \\ncomparison and addition numbers are three and one, respectively. \\n \\n \\n \\nFig. 2: (a) Binary tree.  (b)  Its extension.  Internal nodes are shown as cir cles; \\nexternal nodes are shown as squares. \\n \\nIt helps  in  the  analysis  of  tree  algorithms  to  draw  the  tree's  extension  by \\nreplacing the empty subtrees by special nodes. The extra nodes (shown by \\nlittle squares in  Fig. 2)  are  called  external;  the  original  nodes  ( shown  by  \\nlittle circles) are called internal. By definition, the extension of the em pty \\nbinary tree is a single external node. \\n \\nIt is  easy  to  see  that  the  height  algorithm  makes  exactly  one  addition  for \\nevery internal node of the extended tree, and it makes one comparison to  \\ncheck whether the tree is  empty for every internal and external node. Thus, to  \\nascertain the algorithm's efficiency, we need to know how many external \\nnodes an extended binary tree with n internal nodes can have.  Checking Fi g. 2 \\nand a few similar examples, it is easy to hypothesize that the number of \\nexternal nodes x is always one more than the number of internal nodes n:  \\n   x=n+1        ---------------------------------  (1) \\n \\nTo  prove  this  formula,  consider  the  total  number  of  nodes,  both  internal  \\nand external. Since every node, except the root, is one of the two children of \\nan internal node, we have the equation  \\n2n + 1 = x +n, \\nwhich immediately implies equation (1).  \\nNote  that equation  (1)  also  applies  to  any  nonempty full  binary  tree,  in\", metadata={'source': 'DAA.pdf', 'page': 14}),\n",
       " Document(page_content=\"www.pes.edu \\nwhich, by definition, every node has either zero or two children: for a full \\nbinary tree , n and x denote the numbers of parental nodes and leaves, \\nrespectively. \\n \\nReturning to algorithm Height, the number of comparisons to check whether  \\nthe tree is  empty is  \\nC(n)=n+x=2n+1, \\nwhile the number of additions is  \\nA(n) =  n \\n \\nBinary Tree Traversal \\nThe most important divide-and-conquer algorithms for binary trees are  t he \\nthree classic traversals: preorder, inorder, and postorder. All three traversals \\nvisit nodes of a binary tree recursively, i.e., by visiting the tree's root and its \\nleft and right subtrees. They differ just by the timing of the root's visit:  \\nIn the preorder traversal , the root is visited before the left and right subtrees \\nare visited (in that order).  \\nIn the inorder traversal , the root is visited after visiting its left subtree but \\nbefore visiting the right subtree.  \\nIn the postorder traversal , the root is visited after visiting the left and right \\nsubtrees (in that order). \\n \\n \\nFig. 3: Binary tree and its traversals \\n \\nAlgorithm Inorder(T) \\nif T ≠  Ø  \\n Inorder(T left) \\n print(root of T) \\n Inorder(T right)\", metadata={'source': 'DAA.pdf', 'page': 15}),\n",
       " Document(page_content='www.pes.edu \\nAlgorithm Preorder(T) \\nif T ≠ Ø  \\n print(root of T) \\n Preorder(T left) \\n Preorder(T right) \\n \\nAlgorithm Postorder(T) \\nif T ≠ Ø  \\n Postorder(T left) \\n Postorder(T right) \\n print(root of T)', metadata={'source': 'DAA.pdf', 'page': 16}),\n",
       " Document(page_content=\"www.pes.edu \\n \\n \\n \\n \\n \\n \\n \\nMultiplication of Large Integers and \\nStrassen's Matrix Multiplication  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nDr. Shylaja S S Department of Computer Science and Engineering \\nPES UNIVERSITY \\nUE19CS251 : Design and Analysis of Algorithms (4-0-0-4- 4)\", metadata={'source': 'DAA.pdf', 'page': 17}),\n",
       " Document(page_content=\"www.pes.edu \\nMultiplication of large Integers  \\nSome applications, notably modern cryptology, require manipulation of \\nintegers that are over 100 decimal digits long.  Since such integers are too long  \\nto fit in a single word of a modern computer, they require special treatment. \\nThis practical need supports investigations of algorithms for efficient \\nmanipulation of large integers. In this section, we outline an interesti ng \\nalgorithm for multiplying such numbers. Obviously, if we use the classic  pen-\\nand-pencil algorithm for multiplying two n-digit integers, each of the n digits of \\nthe first number is multiplied by each of the n digits of the second numb er for \\nthe total of n2 digit multiplications.  (If one of the numbers has fewer digits \\nthan the other, we can pad a shorter number with leading zeros to equal their \\nlengths.) Though it might appear that it would be impossible to design an \\nalgorithm with fewer than n2 digit multiplications, it turns out not to be the \\ncase. The miracle of divide-and-conquer comes to the rescue to accomplish  \\nthis feat.  \\nTo  demonstrate  the  basic idea of the  algorithm,  let us  start  with  a  case  of \\ntwo - digit integers, say, 23  and 14. These numbers can be represented as \\nfollows: \\n23 = 2. 101 + 3. 100 and 14 = 1. 101 + 4. 100 \\nNow let us multiply them:  \\n23 * 14 =  (2  . 101 +  3. 100) * (1  . 101 +  4. 100)  \\n=  (2 * 1)102 +  (2 * 4 +  3 * 1)101 +  (3  * 4)100.  \\nThe last formula yields the correct answer of 322, of course, but it uses the \\nsame four digit multiplications as the pen-and-pencil algorithm.  Fortun ately, \\nwe can compute the middle term with just one digit multiplication by taking \\nadvantage of the products 2 * 1 and 3 * 4 that need to be computed anyway:  \\n2 * 4 +  3 * 1 =  (2 +  3)  * (1  +  4)  - 2 * 1- 3 * 4. \\nOf course,  there  is  nothing  special  about  the  numbers  we  just  multipl ied.  \\nFor any pair of two-digit integers a  =  a 1a0  and b =  b 1b0 ,  their product c  can \\nbe computed by the formula  \\nc =a* b =  c 2 102 + c 1101 + c 0 ,  \\nwhere  \\nc2  =  a 1  * b 1 is  the product of their first digits,  \\nc0 =  a 0  * b 0 is  the product of their second digits,  \\nc1  =  (a 1+ a 0)  * (b 1 +  b 0)  - (c2  +c0) is the product of the sum of the a's digits  \\nand the sum of the b's digits minus the sum of c 2  and c 0.\", metadata={'source': 'DAA.pdf', 'page': 18}),\n",
       " Document(page_content=\"www.pes.edu \\nNow we apply this trick to multiplying two n-digit integers a and b where n  is a \\npositive even number. Let us divide both numbers in the middle-after al l, we \\npromised to take advantage of the divide-and-conquer technique. We denote \\nthe first half of the a's digits by a 1 and the second half by a 0; for b, the \\nnotations are b 1 and b 0 , respectively. In these notations, a= a 1a0 implies that \\na= a 110n/2  + a 0 , and b =  b 1b0  implies  that b =  b 110n/2  +  b 0 .  Therefore, taking \\nadvantage of the same trick we used for two-digit numbers, we get \\nc = a*b = (a 110n/2 + a 0) * (b 110n/2 + b 0)  \\n   =  (a 1* b 1)10n  +  (a 1*b0  +  a 0*b1)10n/2  +  (a0*b0)  \\n   =  c 210n + c 110n/2  + c 0 \\nwhere \\nc2 = a 1 * b 1 is the product of their first halves \\nc0= a 0 * b 0 is the product of their second halves \\nc1=(a 1 + a 0) * (b 1+b0) – (c2+c0) is the product of the sum of the a’s halves and \\nthe sum of the b’s halves minus the sum of c 2 and c 0 \\nIf n/2 is even, we can apply the same method for computing the products c 2,  \\nc0, and c 1. Thus, if n is a power of 2, we have a recursive algorithm for \\ncomputing the product of two n-digit integers. In its pure form, the recursion is \\nstopped when n becomes one. It can also be stopped when we deem n small  \\nenough to multiply the numbers of that size directly. \\n \\nHow many digit multiplications does this algorithm make? Since multiplica tion \\nof n-digit numbers requires three multiplications of n/2-digit numbers,  the \\nrecurrence for the number of multiplications M (n)  will be  \\nM(n) =  3M(n/2)  for n  >  1,  M(1) =  1.  \\nSolving it by backward substitutions for n =  2k  yields \\nM(2k) = 3M(2k-1) = 3[3M(2k-2)] = 32M(2k-2)  \\n = ... = 3i M(2k-i) = ... = 3kM(2k-k) = 3k \\nSince k = log2n: M(n) = 3log2n\\n = nlog23\\n = n1.585 \\nStrassen's Matrix Multiplication \\nNow  that  we  have  seen  that  the  divide-and-conquer  approach  can  \\nreduce  the number of one-digit multiplications in multiplying two integers, we \\nshould not be surprised that a similar feat can be accomplished for  multiplying\", metadata={'source': 'DAA.pdf', 'page': 19}),\n",
       " Document(page_content=\"www.pes.edu \\nmatrices.  Such an algorithm was published by V. Strassen in 1969 . The \\nprincipal insight of the algorithm lies in the discovery that we can find the  \\nproduct C of two 2- by-2 matrices A and B with just seven multiplications as \\nopposed to the eight required by the brute-force algorithm. This is \\naccomplished by using the following formulas: \\n \\n[ 𝑐00 𝑐01\\n𝑐10 𝑐11 ] = [ 𝑎00 𝑎01\\n𝑎10 𝑎11 ] * [ 𝑏00 𝑏01\\n𝑏10 𝑏11 ] \\n                       = [ 𝑚1+ 𝑚 4− 𝑚 5+ 𝑚 7 𝑚3+ 𝑚 5\\n𝑚2+ 𝑚 4 𝑚1+ 𝑚 3− 𝑚 2+ 𝑚 6 ] \\nwhere, \\nm1= (a 00 + a 11) * (b 00 + b 11) \\nm2= (a 10 + a 11) * b 00 \\nm3= a 00 * (b 01 - b 11) \\nm4= a 11 * (b 10 - b 00) \\nm5= (a 00 + a 01) * b 11 \\nm6= (a 10 - a 00) * (b 00 + b 01) \\nm7= (a 01 - a 11) * (b 10 + b 11) \\nThus, to multiply two 2- by-2 matrices, Strassen's algorithm makes seven \\nmultiplications and 18 additions/subtractions, whereas the brute-force \\nalgorithm requires eight multiplications and four additions. These numbers \\nshould not lead us to multiplying 2- by-2 matrices by Strassen's algorithm.  Its \\nimportance stems from its asymptotic superiority as matrix order n goes to \\ninfinity.  \\nLet A and B be two n- by-n matrices where n is a power of two.  (If n is not a \\npower of two, matrices can be padded with rows and columns of zeros.)  We \\ncan divide A, B, and their product C into four n /2- by-n /2 submatrices each as \\nfollows: \\n[𝐶00 𝐶01\\n𝐶10 𝐶11] = [𝐴00 𝐴01\\n𝐴10 𝐴11] * [𝐵00 𝐵01\\n𝐵10 𝐵11] \\nIt is not difficult to verify that one can treat these submatrices as numbers to \\nget the correct product. For example, C 00 can be computed either as\", metadata={'source': 'DAA.pdf', 'page': 20}),\n",
       " Document(page_content=\"www.pes.edu \\nA00* B 00 + A 01 * B 10 or  as  M 1 + M 4  - M 5  + M 7  where  M 1,  M 4,  M 5,  and  M 7  are  \\nfound  by Strassen's formulas,  with  the  numbers  replaced  by  the  \\ncorresponding  submatrices.  If the seven products of n /2-by-n/2  matrices  are \\ncomputed  recursively  by  the  same method, we have Strassen's algorithm for \\nmatrix multiplication.  \\nLet us evaluate the asymptotic efficiency of this algorithm. If M(n) is the \\nnumber of multiplications made by Strassen's algorithm in multiplying two  n-\\nby-n matrices (where n is a power of 2), we get the following recurrence \\nrelation for it: \\nM(n)=7M(n/2)  for n> 1,  M(1 )=1. \\nSince n = 2k, \\n  M(2k) = 7M(2k-1) = 7[7M(2k-2)] = 72M(2k-2) = ... \\n       = 7i M(2k-i) ... = 7k M (2k-k) = 7k \\nSince k = log 2n, \\n   M(n)=  7log 2n=  nlog 27  ≈  n2.807  \\nwhich is  smaller than n3  required by  the brute-force algorithm.  \\nSince this saving in the number of multiplications was achieved at the expense \\nof making extra additions, we must check the number of additions A(n) made \\nby Strassen's algorithm. To multiply two matrices of order n > 1, the algorithm \\nneeds to multiply seven matrices of order n/2  and make 18 additions of \\nmatrices of size n/2; when n =  1,  no additions are made since two numbers \\nare simply multiplied.  \\nThese observations yield the following recurrence relation:  \\nA(n) = 7 A(n/2) + 18(n/2)2      for n>1, A(1) =0 \\nAccording to the Master Theorem, A(n) ∈ Θ(nlog\\n27\\n). In other words, the number \\nof additions has the same order of growth as the number of multiplications.  \\nThis puts Strassen's algorithm in Θ (nlog\\n27\\n), which is a better efficiency class than \\nΘ (n3)  of the brute-force method.\", metadata={'source': 'DAA.pdf', 'page': 21}),\n",
       " Document(page_content='Heap and Heap Sort \\n \\nHeap can be defined as a binary tree with keys assigned to its nodes, one key per node, \\nprovided the following two conditions are met: \\n1. The shape property —the binary tree is essentially complete (or simply complete ), i.e., all \\nits levels are full except possibly the last level, where only some rightmost leaves may be \\nmissing. \\n2. The parental dominance or heap property —the key in each node is greater than or equal \\nto the keys in its children. (This condition is considered automatically satisfied for all leaves.) \\n \\nproperties of heaps \\n \\n \\nBottom Up Construction', metadata={'source': 'DAA.pdf', 'page': 22}),\n",
       " Document(page_content='Top Down Construction', metadata={'source': 'DAA.pdf', 'page': 23}),\n",
       " Document(page_content='Red Black Tree \\n \\nIntroduction: \\nA red-black tree is a kind of self-balancing binary search tr ee where each node \\nhas an extra bit, and that bit is often interpreted as the col our (red or black). \\nThese colours are used to ensure that the tree remains balanced  during insertions \\nand deletions. Although the balance of the tree is not perf ect, it is good enough \\nto reduce the searching time and maintain it around O(log n) time, where n is \\nthe total number of elements in the tree. This tree was i nvented in 1972 by \\nRudolf Bayer.  \\nIt must be noted that as each node requires only 1 bit of s pace to store the colour \\ninformation, these types of trees show identical memory footprint to the cl assic \\n(uncoloured) binary search tree.  \\nRules That Every Red-Black Tree Follows:  \\n1. Every node has a colour either red or black. \\n2. The root of the tree is always black. \\n3. There are no two adjacent red nodes (A red node cannot have a r ed \\nparent or red child). \\n4. Every path from a node (including root) to any of its d escendants \\nNULL nodes has the same number of black nodes. \\n \\n \\nMost of the BST operations (e.g., search, max, min, insert,  delete.. etc) take O(h) \\ntime where h is the height of the BST. The cost of thes e operations may become \\nO(n) for a skewed Binary tree. If we make sure that the heigh t of the tree remains \\nO(log n) after every insertion and deletion, then we can guaran tee an upper \\nbound of O(log n) for all these operations. The height o f a Red-Black tree is \\nalways O(log n) where n is the number of nodes in the tree.  \\nComparison with AVL Tree: \\nThe AVL trees are more balanced compared to Red-Black Trees, but th ey may \\ncause more rotations during insertion and deletion. So if your  application \\ninvolves frequent insertions and deletions, then Red-Black t rees should be \\npreferred. And if the insertions and deletions are less freq uent and search is a \\nmore frequent operation, then AVL tree should be preferred o ver Red-Black \\nTree. \\nInteresting points about Red-Black Tree: \\n1. Black height of the red-black tree is the number of black nodes on  a \\npath from the root node to a leaf node. Leaf nodes are also counted  as \\nblack nodes. So, a red-black tree of height h has black height >= h/2. \\n2. Height of a red-black tree with n nodes is h<= 2 log 2(n + 1).', metadata={'source': 'DAA.pdf', 'page': 26}),\n",
       " Document(page_content='3. All leaves (NIL) are black. \\n4. The black depth of a node is defined as the number of black no des \\nfrom the root to that node i.e the number of black ancestors. \\n5. Every red-black tree is a special case of a binary tree. \\n \\n \\nSearch Operation in Red-black Tree: \\nAs every red-black tree is a special case of a binary tree so th e searching \\nalgorithm of a red-black tree is similar to that of a binary tree. \\n \\nsearchElement (tree, val) \\nStep 1: \\nIf tree -> data = val OR tree = NULL \\n    Return tree \\nElse \\nIf val  data \\n        Return searchElement (tree -> left, val) \\n    Else \\n        Return searchElement (tree -> right, val) \\n    [ End of if ] \\n[ End of if ] \\n \\nStep 2: END', metadata={'source': 'DAA.pdf', 'page': 27}),\n",
       " Document(page_content='Applications:  \\n1. Most of the self-balancing BST library functions like map and  set in \\nC++ (OR TreeSet and TreeMap in Java) use Red-Black Tree. \\n2. It is used to implement CPU Scheduling Linux. Completely Fai r \\nScheduler uses it. \\n3. Besides they are used in the K-mean clustering algorithm for reducing \\ntime complexity. \\n4. Moreover, MySQL also uses the Red-Black tree for indexes on tab les. \\n \\nInsertion in Red Black Tree: \\nAlgorithm: \\nLet x be the newly inserted node. \\n1. Perform standard BST insertion and make the colour of newly i nserted \\nnodes as RED. \\n2. If x is the root, change the colour of x as BLACK (Black h eight of \\ncomplete tree increases by 1). \\n3. Do the following if the color of x’s parent is not BLACK  and x is not \\nthe root.  \\na) If x’s uncle is RED  (Grandparent must have been black \\nfrom property  \\n4). (i)  Change the colour of parent and uncle as BLACK.', metadata={'source': 'DAA.pdf', 'page': 28}),\n",
       " Document(page_content='(ii) Colour of a grandparent as RED.  \\n(iii) Change x = x’s grandpare nt, repeat steps 2 and 3 for new x.  \\nb) If x’s uncle is BLACK , then there can be four configurations for \\nx, x’s parent ( p) and x’s grandparent ( g) (This is similar to AVL  Tree)  \\n(i) Left Left Case (p is left child of g and x is left child of p)  \\n(ii) Left Right Case (p is left child of g and x is the right  child of p)  \\n(iii) Right Right Case (Mirror of case i)  \\n(iv) Right Left Case (Mirror of case ii)', metadata={'source': 'DAA.pdf', 'page': 29}),\n",
       " Document(page_content='Final Red Black Tree:', metadata={'source': 'DAA.pdf', 'page': 31}),\n",
       " Document(page_content='Red Black Tree \\n \\nIntroduction: \\nA red-black tree is a kind of self-balancing binary search tr ee where each node \\nhas an extra bit, and that bit is often interpreted as the col our (red or black). \\nThese colours are used to ensure that the tree remains balanced  during insertions \\nand deletions. Although the balance of the tree is not perf ect, it is good enough \\nto reduce the searching time and maintain it around O(log n) time, where n is \\nthe total number of elements in the tree. This tree was i nvented in 1972 by \\nRudolf Bayer.  \\nIt must be noted that as each node requires only 1 bit of s pace to store the colour \\ninformation, these types of trees show identical memory footprint to the cl assic \\n(uncoloured) binary search tree.  \\nRules That Every Red-Black Tree Follows:  \\n1. Every node has a colour either red or black. \\n2. The root of the tree is always black. \\n3. There are no two adjacent red nodes (A red node cannot have a r ed \\nparent or red child). \\n4. Every path from a node (including root) to any of its d escendants \\nNULL nodes has the same number of black nodes. \\n \\n \\nMost of the BST operations (e.g., search, max, min, insert,  delete.. etc) take O(h) \\ntime where h is the height of the BST. The cost of thes e operations may become \\nO(n) for a skewed Binary tree. If we make sure that the heigh t of the tree remains \\nO(log n) after every insertion and deletion, then we can guaran tee an upper \\nbound of O(log n) for all these operations. The height o f a Red-Black tree is \\nalways O(log n) where n is the number of nodes in the tree.  \\nComparison with AVL Tree: \\nThe AVL trees are more balanced compared to Red-Black Trees, but th ey may \\ncause more rotations during insertion and deletion. So if your  application \\ninvolves frequent insertions and deletions, then Red-Black t rees should be \\npreferred. And if the insertions and deletions are less freq uent and search is a \\nmore frequent operation, then AVL tree should be preferred o ver Red-Black \\nTree. \\nInteresting points about Red-Black Tree: \\n1. Black height of the red-black tree is the number of black nodes on  a \\npath from the root node to a leaf node. Leaf nodes are also counted  as \\nblack nodes. So, a red-black tree of height h has black height >= h/2. \\n2. Height of a red-black tree with n nodes is h<= 2 log 2(n + 1).', metadata={'source': 'DAA.pdf', 'page': 32}),\n",
       " Document(page_content='3. All leaves (NIL) are black. \\n4. The black depth of a node is defined as the number of black no des \\nfrom the root to that node i.e the number of black ancestors. \\n5. Every red-black tree is a special case of a binary tree. \\n \\n \\nSearch Operation in Red-black Tree: \\nAs every red-black tree is a special case of a binary tree so th e searching \\nalgorithm of a red-black tree is similar to that of a binary tree. \\n \\nsearchElement (tree, val) \\nStep 1: \\nIf tree -> data = val OR tree = NULL \\n    Return tree \\nElse \\nIf val  data \\n        Return searchElement (tree -> left, val) \\n    Else \\n        Return searchElement (tree -> right, val) \\n    [ End of if ] \\n[ End of if ] \\n \\nStep 2: END', metadata={'source': 'DAA.pdf', 'page': 33}),\n",
       " Document(page_content='Applications:  \\n1. Most of the self-balancing BST library functions like map and  set in \\nC++ (OR TreeSet and TreeMap in Java) use Red-Black Tree. \\n2. It is used to implement CPU Scheduling Linux. Completely Fai r \\nScheduler uses it. \\n3. Besides they are used in the K-mean clustering algorithm for reducing \\ntime complexity. \\n4. Moreover, MySQL also uses the Red-Black tree for indexes on tab les. \\n \\nInsertion in Red Black Tree: \\nAlgorithm: \\nLet x be the newly inserted node. \\n1. Perform standard BST insertion and make the colour of newly i nserted \\nnodes as RED. \\n2. If x is the root, change the colour of x as BLACK (Black h eight of \\ncomplete tree increases by 1). \\n3. Do the following if the color of x’s parent is not BLACK  and x is not \\nthe root.  \\na) If x’s uncle is RED  (Grandparent must have been black \\nfrom property  \\n4). (i)  Change the colour of parent and uncle as BLACK.', metadata={'source': 'DAA.pdf', 'page': 34}),\n",
       " Document(page_content='(ii) Colour of a grandparent as RED.  \\n(iii) Change x = x’s grandpare nt, repeat steps 2 and 3 for new x.  \\nb) If x’s uncle is BLACK , then there can be four configurations for \\nx, x’s parent ( p) and x’s grandparent ( g) (This is similar to AVL  Tree)  \\n(i) Left Left Case (p is left child of g and x is left child of p)  \\n(ii) Left Right Case (p is left child of g and x is the right  child of p)  \\n(iii) Right Right Case (Mirror of case i)  \\n(iv) Right Left Case (Mirror of case ii)', metadata={'source': 'DAA.pdf', 'page': 35}),\n",
       " Document(page_content='Final Red Black Tree:', metadata={'source': 'DAA.pdf', 'page': 37}),\n",
       " Document(page_content=\"2-3 Tree \\na 2–3 tree  is a tree data structure, where every node with children (internal node) has either \\ntwo children (2-node) and one data element or three children (3-nodes) and two data \\nelements. A 2-3 tree is a B-tree of order 3. Nodes on the outside of the tree (leaf nodes) have \\nno children and one or two data elements. 2−3 trees were invented by  John Hopcroft in 1970 \\n2–3 trees are required to be balanced, meaning that each leaf is at the same level. It follows \\nthat each right, center, and left subtree of a node contains the same or close to the same \\namount of data. \\nWe say that an internal node is a 2-node  if it has one data element and two children. \\nWe say that an internal node is a 3-node  if it has two data elements and three  children. \\nWe say that T is a 2–3 tree  if and only if one of the following statements hold:  \\n\\uf0b7 T is empty. In other words, T does not have any nodes. \\n\\uf0b7 T is a 2-node with data element a. If T has left child p and right child q, then \\no p and q are 2 –3 trees of the same height \\no a is greater than each element in p; and \\no a is less than or equal to each data element in q. \\n\\uf0b7 T is a 3-node with data elements a and b, where a < b. If T has left child p, middle \\nchild q, and right child r, then \\no p, q, and r are 2 –3 trees of equal height; \\no a is greater than each data element in p and less than or equal to each \\ndata element in q; and \\no b is greater than each data element in q and less than or equal to each \\ndata element in r. \\nProperties \\n\\uf0b7 Every internal node is a 2-node or a 3-node. \\n\\uf0b7 All leaves are at the same level. \\n\\uf0b7 All data is kept in sorted order. \\nSearching \\nSearching for an item in a 2 –3 tree is similar to searching for an item in a binary search tree. \\nSince the data elements in each node are ordered, a search function will be directed to the \\ncorrect subtree and eventually to the correct node which contains the item. \\n1. Let T be a 2 –3 tree and d be the data element we want to find. If T is empty, \\nthen d is not in T and we're done. \\n2. Let t be the root of T. \\n3. Suppose t is a leaf. \\no If d is not in t, then d is not in T. Otherwise, d is in T. We need no \\nfurther steps and we're done. \\n4. Suppose t is a 2-node with left child p and right child q. Let a be the data \\nelement in t. There are three cases: \\no If d is equal to a, then we've found d in T and we're done.\", metadata={'source': 'DAA.pdf', 'page': 38}),\n",
       " Document(page_content=\"o If  d<a, then set T to p, which by definition is a 2 –3 tree, and go \\nback to step 2. \\no If  d<a, then set T to q and go back to step 2. \\n5. Suppose t is a 3-node with left child p, middle child q, and right child r. \\nLet a and b be the two data elements of t, where a<b. There are four cases: \\no If d is equal to a or b, then d is in T and we're done. \\no If d<a, then set T to p and go back to step 2. \\no If  d<a, then set T to q and go back to step 2. \\no If  d<a, then set T to r and go back to step 2. \\nInsertion \\nInsertion maintains the balanced property of the tree.[5] \\nTo insert into a 2-node, the new key is added to the 2-node in the appropriate order. \\nTo insert into a 3-node, more work may be required depending on the location of the 3-node. \\nIf the tree consists only of a 3-node, the node is split into three 2-nodes with the appropriate \\nkeys and children. \\n \\nThe delete operation \\nDeleting key k is similar to inserting: there is a special case when T is just a single (leaf) node \\ncontaining k (T is made empty); otherwise, the parent of the node to be deleted is found, then \\nthe tree is fixed up if necessary so that it is still a 2-3 tree. \\nOnce node n (the parent of the node to be deleted) is found, there are two cases, depending on \\nhow many children n has: \\ncase 1: n has 3 children \\n\\uf0b7 Remove the child with value k, then fix n.leftMax, n.middleMax, and n's ancestors' \\nleftMax and middleMax fields if necessary.\", metadata={'source': 'DAA.pdf', 'page': 39}),\n",
       " Document(page_content='case 2: n has only 2 children \\n\\uf0b7 If n is the root of the tree, then remove the node containing k. Replace the root node \\nwith the other child (so the final tree is just a single leaf node). \\n\\uf0b7 If n has a left or right sibling with 3 kids, then: \\no remove the node containing k \\no \"steal\" one of the sibling\\'s children \\no fix n.leftMax, n.middleMax, and the leftMax and middleMax fields of n\\'s \\nsibling and ancestors as needed. \\n\\uf0b7 If n\\'s sibling(s) have only 2 children, then: \\no remove the node containing k \\no make n\\'s remaining child a child of n\\'s sibling \\no fix leftMax and middleMax fields of n\\'s sibling as needed \\no remove n as a child of its parent, using essentially the same two cases \\n(depending on how many children n\\'s parent has) as those just discussed \\nThe time for delete is similar to insert; the worst case involves one traversal down the tree to \\nfind n, and another \"traversal\" up the tree, fixing leftMax and middleMax fields along the \\nway (the traversal up is really actions that happen after the recursive call to delete h as \\nfinished).', metadata={'source': 'DAA.pdf', 'page': 40}),\n",
       " Document(page_content='B Tree \\nB Tree is a Self-balancing search tree that maintains sorted data and allows searches, \\nsequential access, insertions, and deletions in logarithmic time . The B-tree generalizes \\nthe binary search tree , allowing for nodes with more than two children. Unlike other self-\\nbalancing binary search trees , the B-tree is well suited for storage systems that read and write \\nrelatively large blocks of data, such as disks. It is commonly used in databases  and file \\nsystems . B-trees were invented by Rudolf Bayer  and Edward M.McCreight  while working \\nat Boeing Research Labs , for the purpose of efficiently managing index pages for large \\nrandom access files. The basic assumption was that indexes would be so voluminous that \\nonly small chunks of the tree could fit in main memory. \\nA B-tree of order m is a tree which satisfies the following properties: \\n1. Every node has at most m children. \\n2. Every non-leaf node (except root) has at least ⌈m/2⌉ child nodes. \\n3. The root has at least two children if it is not a leaf node. \\n4. A non-leaf node with k children contains k − 1 keys.  \\n5. All leaves appear in the same level and carry no information. \\nEach internal node\\'s keys act as separation values which divide its subtrees. For example, if \\nan internal node has 3 child nodes (or subtrees) then it must have 2 keys: a1 and a2. All values \\nin the leftmost subtree will be less than a1, all values in the middle subtree will be \\nbetween a1 and a2, and all values in the rightmost subtree will be greater than a2. \\nInternal nodes \\nInternal nodes are all nodes except for leaf nodes and the root node. They are usually \\nrepresented as an ordered set of elements and child pointers. Every internal node contains \\na maximum  of U children and a minimum  of L children. Thus, the number of elements is \\nalways 1 less than the number of child pointers (the number of elements is between L−1 \\nand U−1). U must be either 2 L or 2 L−1; therefore each internal node is at least half full. The \\nrelationship between U and L implies that two half-full nodes can be joined to make a legal \\nnode, and one full node can be split into two legal nodes (if there\\'s room to push one element \\nup into the parent). These properties make it possible to delete and insert new values into a B-\\ntree and adjust the tree to preserve the B-tree properties. \\nThe root node \\nThe root node\\'s number of children has the same upper limit as internal nodes, but has no \\nlower limit. For example, when there are fewer than L−1 elements in the entire tree, the root \\nwill be the only node in the tree with no children at all. \\nLeaf nodes \\nIn Knuth\\'s terminology, leaf nodes do not carry any information. The internal nodes that are \\none level above the leaves are what would be called \"leaves\" by other authors: these nodes \\nonly store keys (at most m-1, and at least m/2-1 if they are not the root) and pointers to nodes \\ncarrying no information. \\nA B-tree of depth n+1 can hold about U times as many items as a B-tree of depth n, but the \\ncost of search, insert, and delete operations grows with the depth of the tree. As with any \\nbalanced tree, the cost grows much more slowly than the number of elements. \\nSome balanced trees store values only at leaf nodes, and use different kinds of nodes for leaf \\nnodes and internal nodes. B-trees keep values in every node in the tree except leaf nodes.', metadata={'source': 'DAA.pdf', 'page': 41}),\n",
       " Document(page_content='Properties of B-Tree:   \\n1. All leaves are at the same level. \\n2. A B-Tree is defined by the term minimum degree  ‘t’. The value of t depends \\nupon disk block size. \\n3. Every node except root must contain at least (ceiling)([t -1]/2) keys. The root \\nmay contain minimum 1 key. \\n4. All nodes (including root) may contain at most t – 1 keys. \\n5. Number of children of a node is equal to the number of ke ys in it plus 1. \\n6. All keys of a node are sorted in increasing order. T he child between two keys k1 \\nand k2 contains all keys in the range from k1 and k2. \\n7. B-Tree grows and shrinks from the root which is unlik e Binary Search Tree. \\nBinary Search Trees grow downward and also shrink from  downward. \\n8. Like other balanced Binary Search Trees, time comple xity to search, insert and \\ndelete is O(log n). \\n \\n \\n \\n \\nSearching 120 in the given B-Tree', metadata={'source': 'DAA.pdf', 'page': 42}),\n",
       " Document(page_content='Insertion \\n1. Initialize x as root. \\n \\n2) While x is not leaf, do following \\n \\n..a) Find the child of x that is going to be traversed next. Let the child be y. \\n \\n..b) If y is not full, change x to point to y. \\n \\n..c) If y is full, split it and change x to point to one of the two parts of y. If k is smaller than \\nmid key in y, then set x as the first part of y. El se second part of y. When we split y, we \\nmove a key from y to its parent x. \\n \\n3) The loop in step 2 stops when x is leaf. x must have s pace for 1 extra key as we have \\nbeen splitting all nodes in advance. So simply insert k to  x. \\n \\nLet us understand the algorithm with an example tree of  minimum degree ‘t’ as 3 and a \\nsequence of integers 10, 20, 30, 40, 50, 60, 70, 80 and 9 0 in an initially empty B-Tree. \\nInitially root is NULL. Let us first insert 10.', metadata={'source': 'DAA.pdf', 'page': 43}),\n",
       " Document(page_content='Let us now insert 20, 30, 40 and 50. They all will be  inserted in root because the maximum \\nnumber of keys a node can accommodate is 2*t – 1 which is 5. \\n \\nLet us now insert 60. Since root node is full, it will firs t split into two, then 60 will be \\ninserted into the appropriate child. \\n \\nLet us now insert 70 and 80. These new keys will be inserted into the appropriate leaf \\nwithout any split. \\n \\nLet us now insert 90. This insertion will cause a split. T he middle key will go up to the \\nparent. \\n \\nDelete Operation in B-Tree \\nDeletion from a B-tree is more complicated than inse rtion, because we can delete a key \\nfrom any node-not just a leaf —and when we delete a key from an internal node, we will \\nhave to rearrange the node ’s children.', metadata={'source': 'DAA.pdf', 'page': 44}),\n",
       " Document(page_content='The deletion procedure deletes the key k from the s ubtree rooted at x. This procedure \\nguarantees that whenever it calls itself recursively on a node x, the number of keys in x is at \\nleast the minimum degree t . Note that this condition requi res one more key than the \\nminimum required by the usual B-tree conditions, so th at sometimes a key may have to be \\nmoved into a child node before recursion descends to t hat child. This strengthened \\ncondition allows us to delete a key from the tree in o ne downward pass without having to \\n“back up ” (with one exception, which we ’ll explain). You should interpret the following \\nspecification for deletion from a B-tree with the und erstanding that if the root node x ever \\nbecomes an internal node having no keys (this situati on can occur in cases 2c and 3b then \\nwe delete x, and x ’s only child x.c1 becomes the new root of the tree, d ecreasing the height \\nof the tree by one and preserving the property that the root of the tree contains at least one \\nkey (unless the tree is empty). \\n1.  \\n3. If the key k is not present in internal node x, determ ine the root x.c(i) of the appropriate \\nsubtree that must contain k, if k is in the tree at all. If x.c(i) has only t-1 keys, execute step \\n3a or 3b as necessary to guarantee that we descend t o a node containing at least t keys. \\nThen finish by recursing on the appropriate child of x. \\n    a) If x.c(i) has only t-1 keys but has an immediate s ibling with at least t keys, give x.c(i) \\nan extra key by moving a key from x down into x.c(i) , moving a key from x.c(i) ’s \\nimmediate left or right sibling up into x, and moving t he appropriate child pointer from the \\nsibling into x.c(i). \\n    b) If x.c(i) and both of x.c(i) ’s immediate siblings have t-1 keys, merge x.c(i) with o ne \\nsibling, which involves moving a key from x down into the new merged node to become \\nthe median key for that node. \\nIf the key k is in node x and x is a leaf, delete the k ey k from x. \\n2. If the key k is in node x and x is an internal node, do the following. \\n    a) If the child y that precedes k in node x has at leas t t keys, then find the predecessor k0 \\nof k in the sub-tree rooted at y. Recursively delete k0, and replace k by k0 in x. (We can \\nfind k0 and delete it in a single downward pass.) \\n    b) If y has fewer than t keys, then, symmetrically, e xamine the child z that follows k in \\nnode x. If z has at least t keys, then find the succ essor k0 of k in the subtree rooted at z. \\nRecursively delete k0, and replace k by k0 in x. (W e can find k0 and delete it in a single \\ndownward pass.) \\n     c) Otherwise, if both y and z have only t-1 keys, merge  k and all of z into y, so that x \\nloses both k and the pointer to z, and y now contain s 2t-1 keys. Then free z and recursively \\ndelete k from y. \\nSince most of the keys in a B-tree are in the leaves,  deletion operations are \\nmost often used to delete keys from leaves. The recur sive delete procedure \\nthen acts in one downward pass through the tree, witho ut having to back up. \\nWhen deleting a key in an internal node, however, the procedure makes a \\ndownward pass through the tree but may have to return to the node from \\nwhich the key was deleted to replace the key with its pre decessor or \\nsuccessor', metadata={'source': 'DAA.pdf', 'page': 45})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"DAA.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the question based on the context below. If you can't answer the question, reply \"I dont know\".\n",
      "\n",
      "Context: this is a context\n",
      "Question: this is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't answer the question, reply \"I dont know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(question=\"this is a question\", context=\"this is a context\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! Based on the context provided, your name is Samarth.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"question\": \"What is my name\",\n",
    "    \"context\": \"the name i was given is samarth\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Applications:  \\n1. Most of the self-balancing BST library functions like map and  set in \\nC++ (OR TreeSet and TreeMap in Java) use Red-Black Tree. \\n2. It is used to implement CPU Scheduling Linux. Completely Fai r \\nScheduler uses it. \\n3. Besides they are used in the K-mean clustering algorithm for reducing \\ntime complexity. \\n4. Moreover, MySQL also uses the Red-Black tree for indexes on tab les. \\n \\nInsertion in Red Black Tree: \\nAlgorithm: \\nLet x be the newly inserted node. \\n1. Perform standard BST insertion and make the colour of newly i nserted \\nnodes as RED. \\n2. If x is the root, change the colour of x as BLACK (Black h eight of \\ncomplete tree increases by 1). \\n3. Do the following if the color of x’s parent is not BLACK  and x is not \\nthe root.  \\na) If x’s uncle is RED  (Grandparent must have been black \\nfrom property  \\n4). (i)  Change the colour of parent and uncle as BLACK.', metadata={'source': 'DAA.pdf', 'page': 28}),\n",
       " Document(page_content='Applications:  \\n1. Most of the self-balancing BST library functions like map and  set in \\nC++ (OR TreeSet and TreeMap in Java) use Red-Black Tree. \\n2. It is used to implement CPU Scheduling Linux. Completely Fai r \\nScheduler uses it. \\n3. Besides they are used in the K-mean clustering algorithm for reducing \\ntime complexity. \\n4. Moreover, MySQL also uses the Red-Black tree for indexes on tab les. \\n \\nInsertion in Red Black Tree: \\nAlgorithm: \\nLet x be the newly inserted node. \\n1. Perform standard BST insertion and make the colour of newly i nserted \\nnodes as RED. \\n2. If x is the root, change the colour of x as BLACK (Black h eight of \\ncomplete tree increases by 1). \\n3. Do the following if the color of x’s parent is not BLACK  and x is not \\nthe root.  \\na) If x’s uncle is RED  (Grandparent must have been black \\nfrom property  \\n4). (i)  Change the colour of parent and uncle as BLACK.', metadata={'source': 'DAA.pdf', 'page': 34}),\n",
       " Document(page_content='(ii) Colour of a grandparent as RED.  \\n(iii) Change x = x’s grandpare nt, repeat steps 2 and 3 for new x.  \\nb) If x’s uncle is BLACK , then there can be four configurations for \\nx, x’s parent ( p) and x’s grandparent ( g) (This is similar to AVL  Tree)  \\n(i) Left Left Case (p is left child of g and x is left child of p)  \\n(ii) Left Right Case (p is left child of g and x is the right  child of p)  \\n(iii) Right Right Case (Mirror of case i)  \\n(iv) Right Left Case (Mirror of case ii)', metadata={'source': 'DAA.pdf', 'page': 35}),\n",
       " Document(page_content='(ii) Colour of a grandparent as RED.  \\n(iii) Change x = x’s grandpare nt, repeat steps 2 and 3 for new x.  \\nb) If x’s uncle is BLACK , then there can be four configurations for \\nx, x’s parent ( p) and x’s grandparent ( g) (This is similar to AVL  Tree)  \\n(i) Left Left Case (p is left child of g and x is left child of p)  \\n(ii) Left Right Case (p is left child of g and x is the right  child of p)  \\n(iii) Right Right Case (Mirror of case i)  \\n(iv) Right Left Case (Mirror of case ii)', metadata={'source': 'DAA.pdf', 'page': 29})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"what is master theorem?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the master theorem?\n",
      "Answer: Based on the provided context, the Master Theorem is a data structure used in Red-Black Tree insertion algorithm. It is a way of categorizing the insertion operations based on the color of the tree and the position of the newly inserted node. The Master Theorem has three cases, which depend on the color of the grandparent of the newly inserted node. These cases are:\n",
      "\n",
      "1. Left Left Case: If the grandparent is RED and the newly inserted node is left child of its parent, then the algorithm changes the colors of the grandparent, parent, and newly inserted node to BLACK, RED, and BLACK, respectively.\n",
      "2. Left Right Case: If the grandparent is RED and the newly inserted node is right child of its parent, then the algorithm changes the colors of the grandparent, parent, and newly inserted node to BLACK, RED, and BLACK, respectively.\n",
      "3. Right Right Case: If the grandparent is BLACK, then there are four possible configurations for the newly inserted node, its parent, and its grandparent (similar to AVL Tree). The algorithm changes the colors of the grandparent, parent, and newly inserted node to BLACK, RED, and BLACK, respectively.\n",
      "\n",
      "The Master Theorem is used in Red-Black Tree insertion algorithm to efficiently handle the color changes that occur during insertion operations.\n",
      "\n",
      "Question: what is quick sort?\n",
      "Answer: Based on the provided context, Quick Sort is an algorithm for sorting data structures using a divide-and-conquer approach. It is based on the Red-Black Tree data structure, which is used to implement CPU Scheduling in Linux and other applications.\n",
      "\n",
      "Question: what is merge sort?\n",
      "Answer: Based on the provided context, I can answer the question as follows:\n",
      "\n",
      "Merge sort is a sorting algorithm that uses the Red-Black tree data structure to implement the merge step of the merge sort algorithm. The Red-Black tree is used to store the sorted list of elements after each level of merging, and the algorithm performs the merging operation by iterating over the nodes of the Red-Black tree, promoting black nodes to red and vice versa.\n",
      "\n",
      "In more detail, the merge sort algorithm uses the following steps:\n",
      "\n",
      "1. Divide the list of unsorted elements into two halves.\n",
      "2. Perform a recursive merge sort on each half of the list.\n",
      "3. Once both halves are sorted, perform a merge operation by iterating over the nodes of the Red-Black tree, promoting black nodes to red and vice versa.\n",
      "4. The resulting sorted list is stored in the Red-Black tree.\n",
      "\n",
      "The use of the Red-Black tree in merge sort provides several advantages, including:\n",
      "\n",
      "* O(log n) time complexity for each level of merging, where n is the number of elements in the list.\n",
      "* A stable sorting algorithm, meaning that the order of equal elements is preserved.\n",
      "* A data structure that can efficiently handle large lists by taking advantage of the properties of the Red-Black tree.\n",
      "\n",
      "In summary, merge sort is a sorting algorithm that uses the Red-Black tree to implement the merge step of the algorithm, providing efficient and stable sorting capabilities for large lists of elements.\n",
      "\n",
      "Question: what is a red black tree?\n",
      "Answer: A Red-Black Tree is a type of self-balancing binary search tree. It is used in various applications such as CPU scheduling, K-means clustering, and indexing in MySQL. The tree is designed to maintain a balance between the number of nodes with colors RED and BLACK, which ensures that the tree remains roughly balanced and efficient for insertion, deletion, and search operations.\n",
      "\n",
      "In a Red-Black Tree, each node has a color (RED or BLACK) and satisfies the following properties:\n",
      "\n",
      "1. Each node has at most two children (a left child and a right child).\n",
      "2. The parent of a node is either RED or BLACK.\n",
      "3. The grandparent of a node is either RED or BLACK.\n",
      "4. If a node's uncle is RED, then the node's grandparent must be BLACK.\n",
      "\n",
      "The insertion algorithm for a Red-Black Tree involves performing standard BST insertion and changing the color of the newly inserted node to RED. If the newly inserted node is the root of the tree, then the color of the root is changed to BLACK. If the newly inserted node is not the root, then the colors of its parent and uncle are changed to BLACK if necessary.\n",
      "\n",
      "The Red-Black Tree has several advantages over other types of binary search trees, including:\n",
      "\n",
      "1. Efficient insertion, deletion, and search operations.\n",
      "2. Balanced tree structure, which means that the height of the tree is roughly proportional to the number of nodes it contains.\n",
      "3. Good worst-case time complexity for insertion, deletion, and search operations.\n",
      "\n",
      "Overall, a Red-Black Tree is a useful data structure in computer science, particularly in applications where efficiency and balance are important.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is the master theorem?\",\n",
    "    \"what is quick sort?\",\n",
    "    \"what is merge sort?\",\n",
    "    \"what is a red black tree?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
